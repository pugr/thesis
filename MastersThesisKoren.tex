\documentclass[12pt, oneside, a4paper]{book}

\renewcommand{\baselinestretch}{1.25}
\setlength{\parskip}{5pt}
\setlength{\parindent}{0pt}

\input{_includes}

%\usepackage{pgf-blur}

% tick
\newcommand{\tick}{\ding{52}}
\newcommand{\fail}{\ding{55}}

% chapters style
\definecolor{gray75}{gray}{0.75}
\newcommand{\hsp}{\hspace{20pt}}
\titleformat{\chapter}[hang]{\Huge\bfseries}{\thechapter\hsp\textcolor{gray75}{|}\hsp}{0pt}{\Huge\bfseries}
\titlespacing{\chapter}{0pt}{0pt}{1cm}

\begin{document}
\inputencoding{utf8}

% *************** Front matter ***************
\frontmatter

\begin{titlepage}

\setlength{\textwidth}{15cm}
\addtolength{\hoffset}{-1.7cm}
\addtolength{\voffset}{-1.8cm}

\vbox to 150mm{

{\fontfamily{phv}\selectfont

\makebox[\textwidth][c]{University of West Bohemia}

\vspace{0.5cm}

\makebox[\textwidth][c]{Faculty of Applied Sciences}

\vspace{0.5cm}

\makebox[\textwidth][c]{Department of Computer Science and Engineering}

\vspace{6cm}

\makebox[\textwidth][c]{\LARGE{\textbf{DIPLOMA THESIS}}}

\vspace{13.3cm}

\makebox[\textwidth][c]{Pilsen, 2013 \hspace{6.5cm} Jan Koreň}



\vss
}


}

\end{titlepage}

\begin{titlepage}

\font\uni=cmr17 at 22pt
\font\dip = cmb10 at 28pt
\font\nam = cmr17 at 16pt

\addtolength{\voffset}{-1.8cm}

\vbox to 150mm{



\makebox[\textwidth][c]{\uni University of West Bohemia}

\vspace{0.5cm}

\makebox[\textwidth][c]{\uni Faculty of Applied Sciences}

\vspace{0.5cm}

\makebox[\textwidth][c]{\uni Department of Computer Science and Engineering}

\vspace{3.5cm}

\makebox[\textwidth][c]{\dip Diploma Thesis}

\vspace{2.5cm}

\makebox[\textwidth][c]{\dip Fulltext search in the database }
\makebox[\textwidth][c]{\dip and in texts of social networks}

\vspace{12.5cm}

\makebox[\textwidth][c]{\nam Pilsen, 2013 \hspace{6.5cm} Jan Koreň}



\vss



}

\end{titlepage}

\thispagestyle{empty}


\section*{Declaration}

I hereby declare that this diploma thesis is completely my own work
and that I used only the cited sources. 


\date{Pilsen, Jan Koreň}

\pagebreak{}


\thispagestyle{empty}

\chapter*{Acknowledgements}

I would like to thank to...

\pagebreak{}


\thispagestyle{empty}


\chapter*{Abstract}


This thesis is focused on design and implementation of the full text
search in EEG/ERP Portal. This full text search capability includes
searching text in the EEG/ERP Portal database as well as in related
data from social networks, such as LinkedIn or Facebook. With the
development of the Portal and the increasing amount of processed data,
a proper full text search mechanism for information retrieval is necessary
for improving the user experience by enabling efficient do\-cu\-ment retrieval. 

\pagebreak{}

%\vspace{5mm}\\
{\bf Keywords}:
Lucene, Solr, full text search

\thispagestyle{empty}

\vfill\eject

\tableofcontents

% *************** Main matter ***************
\mainmatter

\pagestyle{plain}

\input{chapters/introduction}



\part{Theoretical Part}

Information and efficient access to it form an essential part of life in a modern society. 
Computing and related technologies have changed the ways textual information is stored, searched and retrieved. 
The amount of information has grown rapidly in the last few years due to the information explosion caused mainly by the World Wide Web. 
The result is that nowadays, people are exposed to much more information than they used to be. 
Simple categorization of documents made by humans, although doable, is no longer an efficient method of storing data to be searched. 
Except for the fact that this activity is very time-consuming, it can also be automated. 
In order to manage such amount of data and obtain relevant information within a reasonable time period, new powerful techniques operating on vast collections of data were needed.

The problematics of searching relevant information in large data sets has been an objective of a detailed research for more than sixty years. 
The aim of the following parts is to introduce some fundamental concepts of information retrieval, a discipline including the full text search, in the context of full text search and compare these concepts with similar principles used in the relational and
NoSQL database worlds.

The latter chapter describes available open-source full-text engines.



\input{chapters/fulltextSearch}

\input{chapters/searchEngines}

\input{chapters/eegPortal}




\part{Practical Part}


\chapter{Analysis}

\section{Current State of Full Text Search}

By using Hibernate Search to implement full text search, the application
is enforced to use Hibernate or JPA for data persistence. Using any other
technologies than these two results in a malfunctioning application.
The main drawback of using Hibernate Search in our case the inability to index
data from different sources than the database. Since one of the requirements
is to enable searching data from social networks, the current solution
can hardly fulfill this requirement. 

Hibernate search provides the annotation interface which serves for
indexing purposes. A subset of these annotations is used to mark indexed
entities and fields withing them which should form the document in
the index. Next, a few field annotations enable to configure how the
fields are later processed by specifying analyzers to be applied on
those fields.

We do not have such control over indexed data (?) rozepsat

In the current state the indexed data are stored in memory. This means
that the index must be created each time the application starts since
after restarting the application, all indexed data are lost.

Indexed data are kept in an in-memory index, so these data are lost every time the application server stops. This is the reason why data in the database are indexed right after the application starts running.

Hibernate Search uses Lucene as a search engine which handles indexing
and full text search. The created classes in the EEG/ERP Portal application
manipulate straightly with the Lucene API which is considered low-level
for these purposes and results in unexpected behavior of the full
text search in some cases (e.g. text highlighting of a subset of found
results).

Full text search engine, apart from its dependency on Hibernate, is
tightly coupled with the whole application. With the growing amount
of indexed data, the full text search performance might go down as
with limited amount of memory there is not much space for scaling.


\subsection{Database Search}


\section{Desired Improvements of Full Text Search}


\subsection{Social Network Search}

Since Hibernate Search is used as a technology responsible, aside others, for data indexing, only the data stored in the relational database are and can be indexed. However, articles and other information found in the LinkedIn group of EEG/ERP Portal have no direct connection to the database, so Hibernate Search cannot reach these data and therefore cannot save them to the index it keeps. A possible solution which partially solves this problem would be to keep duplicate records about the articles in the EEG/ERP Portal database. Apart from the problem with keeping three copies of basically the same information (the original article published on LinkedIn, its copy in the database and its representation in the Lucene index), those articles published directly from LinkedIn and not from the EEG/ERP Portal via a form would not be noticed by the application and their corresponding documents in the index would not exist.


\section{Current state of integration with social networks}

Currently, EEG/ERP Portal is successfully integrated with its LinkedIn group. A user can use the Portal to publish and see LinkedIn articles straight. To access to LinkedIn, a user must be authorized via the OAuth security protocol.


\section{Choice of full text search solution}

From the search engines listed in previous parts of the thesis and technologies on which EEG/ERP Portal is based, the choice of the search engine can be restricted by the following criteria:

\begin{itemize}
	\item speed - Based on the comparison made in chapter 3, the most performant search engines are ... and Lucene and all those built on Lucene.
	\item integration with EEG/ERP Portal - EEG/ERP Portal is based on Java technologies and this is why search engines providing Java API are easier to be integrated to the working infrastructure.
	\item other features and extensions - Because full text search
engines as they are take care of indexing and searching data, it is desirable to have a set of ``helper tools'' (such as result highlighting, faceted search, synonym search and more-like-this search) to make building full text search easier. It is considered natural for the end user to use the full text search with some of such tools implemented. 
	\item independence on data sources - The chosen search engine must be able to accept data from various sources and not be limited to only one specific data source, such as relational database. The reason behind this is the mentioned need to index LinkedIn articles as well as to enable possible further indexing scenarios in the future, such as indexing .pdf or XML files.
	\item independence on other technologies - This criterion means that the search engine should not rely on a specific technology to be used. Dependence of Hibernate Search on Hibernate or ... on MySQL may serve as examples of the search engines which perform well is the conditions are met, but cannot work if not. 
	\item community - Numerous and active developer community also plays a big role in the final choice. The bigger community around the search engine is, the higher is the chance that the engine development will not stop early, new features will be introduced and that found bugs will be resolved quickly. Although one-man projects can look very promising and their future development is more likely to be managed by still growing community, 
A well documented project, many available tutorials and active user groups are a good sign of project stability and ensure that there will be someone ready to help to solve problems.

\end{itemize}

The search engines were evaluated based on these criteria. This evaluations can be seen for reasons of clarity in table \ref{tab:ComparisonOfFullTextSearchEngines}

TODO MODIFY the table.


\begin{table}
	\centering
		\begin{tabular}{|c|c c c c c|}
		\hline

		\textbf{Search Engine } & \textbf{Speed} & \textbf{Integration} & \textbf{Extension} & \textbf{Independence} & \textbf{Community} \\
		\hline
		Indri & Excellent & \tick & \fail & \tick & 1 \\
		\hline
		Sphinx & Excellent & \tick & \fail & \tick & 3 \\
		\hline
		Lucene & Excellent & \tick & \tick & \tick & 5 \\
		\hline
		Zettair & Excellent & \fail & \fail & \tick & 1 \\
		\hline
		Xapian & Good & \tick & \tick & \tick & 1 \\ 

		\hline
		\end{tabular}
	\caption{Comparison of Full Text Search Engines}
	\label{tab:ComparisonOfFullTextSearchEngines}
\end{table}


	
\section{How Solr Works}


\section{Installation}

The whole Solr distribution can be downloaded from the Solr website \cite{SolrHome}. It comes in a form of a .war archive. Apart from the Solr application itself, it also includes ...

HttpSolrServer and EmbeddedSolrServer


%\section{Solr and NoSQL}


\section{Solr Index}

Todo


\section{Ways of Indexing in Solr}

kecy na zacatek.

\subsection{SolrJ}

Solr provides a Java API for integration of Java applications with the Solr server called SolrJ API.

\subsection{DataImportHandler}

Todo

...

DataImportHandler vs. SolrJ

DataImportHandler (DIH) offers fast indexing of data from relational databases. 
The extraction of data to be added to the index as documents is based on creating custom SQL results. 
The drawback of DIH is its inflexibility. 
After each change of the relational schema the SQL queries which fetch the indexing data must be changed accordingly. 
Also the logic in the form of the SQL queries is out of reach of the EEG/ERP Portal application logic since the queries must be written in a special configuration file managed by the Solr server.

Disadvantages:
\begin{itemize}


\item Data model changes - For indexing of new data and enabling delta imports, it is necessary to add a new column to each table we wish to index. The column contains a timestamp for each record, holding the value of time of the last indexing.

\item After deletion of data in the database, certain actions must be taken to enable DIH to reflect these changes in the index. 
One way to do this is to browse the documents in the index and compare their ids with ids of their matching database records. If a document has no equivalent database record, it means that the record was deleted. After the comparison, redundant documents in the index are identified. This approach does not perform well.
Another way is to add an extra column that captures deletion times. Deletion times together with the record identification are added before the physical deletion of records. DIH can then get all records from the last executed crawl. An appropriate function and before delete trigger must be created for this solution. 

\item if the database schema changes - affected SQL queries must be rewritten to make them work again. SOLR schema changes is related to this matter as well. 

\end{itemize}

SolrJ - possibilities:
\begin{itemize}
\item @Field annotation SolrJ API in combination with the \texttt{addBean()} and \texttt{addBeans()} methods.
The usage of provided annotations brings a problem of ambiguity of mapped objects. Documents stored in the Solr index must possess an identifier which is unique across all stored documents. Object ids are unique only in the class scope, so global uniqueness is not ensured. (pouziti Solr UUID nebylo
prostreleno). It is true that this way of making input documents is clear and easy. Unfortunately, its usage for more advanced scenarios is limited. This limitation lies in the inability to use the @Field annotation for fields which are Java objects and collections. Annotating just primitive or String value fields does not offer 

\item aspects - they are suitable for injecting cross-cutting concerns such as logging and database transactions to avoid spreading the same lines of code across the whole application. 
In our case, the existing need for indexing domain objects can be realized by simply enriching the base DAO methods responsible for creating, updating and deleting an object. This way the indexing calls happen only in a few known places in code. Introducing an extra aspect for this situation is more likely to be overkill, not to mention added complexity in debugging.

\item Integration with Hibernate Search and using its annotation mechanisms
The core idea of this proposed alternative is to use Hibernate Search for the initial phase of indexing, which includes creating input documents for indexing which happens automatically. The created documents would be handed over to Solr.
But - there would be an extra dependency on another technology. Besides, to cover indexing of data not present in the database another indexing mechanisms for these kinds of data would have to be created anyway. 

\item Custom annotation mechanism. By using the possibilities offered by Java Reflection, one cannot be limited by offered solutions and create a new one that overcomes found problems. There are some challenges, but it gives more freedom than previously proposed solutions. It would be desirable to create a solution universal for all domain objects. Inspired by the provided SolrJ annotations. It also involves creating a custom code to implement generating a unique id for each created document.  Bylo by vyuzito reflexe a zajistena

\item filter - web.xml
\end{itemize}

\section{Integration}

Todo


\chapter{Index design}

Je dulezite si pri navrhu indexu uvedomit, ze dokumenty
ulozene v indexu zachycuji informace, ktere jsou vyhledavany. Je proto
nutne znat poradne povahu a ucel fulltexoveho vyhledavani
v dane domene. Tento fakt je pro indexaci dokumentu zvlast
dulezity, protoze dokumenty ukladane v indexu maji jista specifika
a omezeni, ktere jsou rozebrany v nasledujicich odstavcich.


\section{Search Requirements}

The current version of EEG/ERP Portal uses Hibernate Search to implement
the fulltext search feature. This framework is sufficient for dealing
with input data coming only from a database data source. This is because
of the fact that the technology is based on Hibernate, a powerful
object-relational mapping framework. Hibernate Search uses internally
Lucene and Solr analyzers to provide the full text search itself.
However, the current implementation suffers from several issues which
are desired to be fixed. The highlighting functionality does not work
as expected in certain cases. 


\section{Index Structure Specifics}

Compared to traditional relational databases, the Solr index lacks
the possibility to create more structured content. An analogy to the
index would be a single large table in the relational world. As a
relational table, index structure is flat and does not allow nesting
documents to form hierarchical structures as in the case of document
databases like MongoDB.

...Index ma plochou strukturu, neumoznuje vnorovat dokumenty
do sebe, jako v pripade dokumentovych databazi. lze vsak imitovat
relaci 1:N pomoci multivalued fields, ktere jsou ulozeny ve forme
pole.

Reprezentaci dokumentu lze vyresit nekolika zpusoby:
\begin{itemize}
\item Denormalizace relacnich dat
\item Od Solr 4 podpora join funkce. Dost omezene oproti SQL joinu
\item nekolik po sobe nasledujicich dotazu, nasledujici dotaz
bere jako vstup vysledky predchoziho dotazu.
\end{itemize}

\subsection{Eventual consistency}

za pouziti real-time indexovani. po insertu, updatu, deletu dochazi
k indexaci prislusneho dokumentu.

Sekvence insert/update/delete a indexace neprobiha atomicky jako transakce.
Pri chybe muze dojit k nekonzistentnimu stavu (data ulozena,
indexovana -> OK; data ulozena, ale neindexovana -> nebudou vyhledatelna;
data neulozena, indexovana -> nalezeny neodpovidajici nebo neexistujici
vysledky; data neulozena, neindexovana -> DB rollback, puvodni
zaznam lze vyhledat . Osetreni konzistence v techto pripadech
byva netrivialni zalezitost, casto realne resitelna jen
castecne. Beznym resenim nekonzistentnich stavu
je je ignorovat, protoze nakonec budou po dalsi naplnnovani reindexaci
odstraneny. Tim je dosazeno. tzv. eventual consistency.


\chapter{Fulltext Search Design}

Previous parts of the thesis shown that documents in index and records
stored in relational database tables serve to different purposes and
use cases und therefore must be treated differently. The treatment
in this context means storing, structuring and saving the data.

Being aware of the index structure and its specifics, it is almost
always necessary to make a kind of transformation from the relational
to the index form in order to make the full text search work properly
and efficiently. There are several ways to accomplish this goal, one
of them involves flattening the relational structure. This leads to
denormalized data which is mostly unacceptable in relational databases,
but since index is quite different from databases, denormalization
is even desired. The main thing one should know when designing an index
structure is to know which search results a user expects and how the
final representation of full text results should look like.

Pokud bychom zvolili pristup of direct 1:1 mapping of entities to the indexu,
dostaneme pro kazdou entitu odpovidajici typ dokumentu, ktery obsahuje
zvolene indexovane polozky pro danou entitu. 


\chapter{Implementation}


\section{Java Reflection}

Reflection is the ability to inspect the code and make its modifications at runtime. It is a feature that makes languages like Java more dynamic. Its heavy usage can be found especially in modern frameworks such as Spring or Hibernate, that both use reflection for instantiating classes from information in configuration files. 

A very common use case of reflection in Java is the usage with annotations. This combination opens many possibilities of manipulating class metadata. In \textit{JUnit 4}, for example, the \texttt{@Test} annotation was introduced. The JUnit framework looks up all methods marked by this marker annotation and call them in each execution of running unit tests.

The root class of the Java object hierarchy, the Object class, has the \texttt{getClass()} method providing the corresponding \texttt{Class} object, meaning that all Java classes can be invoked or inspected by means of reflection.

%Co to je, na co se pouziva, jak se pouziva v implementaci
%
%The name reflection is used to describe code which is able to inspect other code in the same system (or itself).
%
%One very common use case in Java is the usage with annotations. JUnit 4, for example, will use reflection to look through your classes for methods tagged with the @Test annotation, and will then call them when running the unit test.
%
 %The ability to inspect the code in the system and see object types is Type Introspection. Reflection is then the ability to make modifications at runtime by making use of introspection.
%
 %For example, all objects in Java has the method getClass, which lets you determine its class even if you don't know it at compile time (like if you declared it as Object) - this might seem trivial, but such reflection is not by default possible in less dynamic languages such as C++.
%
%Take for example your typical web.xml file. This will contain a list of servlet elements, which contain nested servlet-class elements. The servlet container will process the web.xml file, and create new a new instance of each servlet class through reflection.
%
%Reflection is important since it lets you write programs that does not have to "know" everything at compile time, making them more dynamic, since they can be tied together at runtime. The code can be written against known interfaces, but the actual classes to be used can be instantiated using reflection from configuration files.
%
%Lots of modern frameworks uses reflection extensively for this very reason. the most comprehensive example is Spring which uses reflection to create its beans, and for its heavy use of proxies
%
%It's useful in a lot of situations. Everywhere you want to be able to dynamically plug in classes into your code. Lot's of object relational mappers use reflection to be able to instantiate objects from databases without knowing in advance what objects they're going to use. Plug-in architectures is another place where reflection is useful. Being able to dynamically load code and determine if there are types there that implement the right interface to use as a plugin is important in those situations.

\section{Collecting LinkedIn Data}

REST

leverages existing technologies - HTTP GET update - POST or PUT. It taken the advantage of the HTTP protocol itself to describe the action that should be taken of a given resource.

\subsection{LinkedIn REST API}

LinkedIn provides REST API to access to various information. \textit{Representational State Transfer} (REST) is pragmatically defined in \cite{REST:Introduction} as \textit{``a set of principles that define how Web standards, such as HTTP and URIs, are supposed to be used''}.
Internal domain model of LinkedIn, which inludes entities like people, companies and jobs, is mapped to REST resources. Resource is something that can be identified by URL. For example, 	
Required information one wishes to obtain can be specified by URI parameters in the JSON format. 

\subsection{Spring Social}

EEG/ERP Portal uses Spring Social to interact with LinkedIn. 
Spring Social offers a bunch of methods to interact with LinkedIn which wrap the existing LinkedIn REST API calls. 
They give a user a set of default fields that are appropriate for some use cases. 
Although it is very convenient to have such abstraction layer, sometimes there is a need to obtain other fields. 
In such cases, a custom LinkedIn REST call must be created.
The call can contain field selectors which are used to specify which fields to return in the response.
The following example shows the usage of field selectors in the REST call which gets full information about a LinkedIn article published in the EEG/ERP Portal group:

...

If the Spring Social method is called, some information, such as article summary and its time stamp, is missing. 



Indexers

An annotation interface was created to cover indexing data. 
Pro efektivni
searching bylo potreba indexovat dokumenty, ktere represent
denormalizoed data. Bylo nutne rozlisit rodicovske objekty a
podrazene objekty, ktere rodicovske objekty obsahuji jako
property nebo ve svych kolekcich. Prikladem muze byt rodicovsky
objekt experiment, ktery obsahuje kolekci podrazenych hardware
objektu.


\section{Collecting database data}

% HOTOVO

\subsection{Hibernate loading strategies}

Hibernate provides two strategies of fetching data from the database to their object representations. 
As an analogy to database relationships, the POJO objects can maintain associations to other objects. 
The strategies differ in the way they treat these object associations, both having their advantages and disadvantages. 

The first possible way is to load all associated object collections at the same time when the database record corresponding to the object is fetched. 
This is called \textit{eager loading}. 
The second way is to return only the data belonging directly to the object and not to fetch the collections until they are required to be fetched. 
This approach is known as \textit{lazy loading}. 

Lazy loading is generally considered a preferred way in most of the applications.
The main reason behind this is performance.
When a certain POJO object (actually the underlying table record) is accessed, it is sufficient for most of the use cases to fetch only the fields of primitive types, because associated collections are not needed. 
By using lazy loading in such scenarios, many unnecessary join and select operations are often avoided. 
In the end, the operation savings reflect both in lower time and memory costs which results in faster application responses. 

On the contrary, overusing eager loading can considerably slow down the application and may lead to the \textit{n+1 selects problem} described e.g. in \cite{HibernateInAction:2004}.	
However, there are many reasonable situations when all associated object data have to be always available. 
As an example, one can imagine a requirement to always display writers together with all the books they have written. 
Then it is completely legitimate to apply eager loading to load the books for each of their authors, since there is a certainty.

There are also cases when it is desired to load otherwise lazily initialized collections eagerly.
Fortunately, there are several ways of how to enforce eager loading in Hibernate.

To understand how lazy loading works in Hibernate, it is important to briefly explain the dynamic proxy pattern. 

\subsubsection{Dynamic proxy}

Hibernate uses dynamic proxies to implement lazy loading of object properties. 
The famous \textit{``Design Patterns''} book written by \textit{Gang of Four} (GOF) \cite{GOF:DesignPatterns} describes the proxy pattern the following way:

\begin{quote}		
\textit{``Allows for object level access control by acting as a pass through entity or a placeholder object.''}
\end{quote}


\begin{figure}[h]
	\centering
		\includegraphics[width=0.7\textwidth]{figures/proxy.eps}
	\caption{Proxy Design Pattern.}
	\label{fig:proxy}
\end{figure}


When an object is required to be loaded, some of its object properties are not actually fetched from the database. Instead, they are represented by their corresponding proxy objects. 
The proxy object is usually referred to as \textit{stub} and does not hold any actual information. 
Its only ability is to call the real object it represents. The UML diagram depicted in figure \ref{fig:proxy} helps visualize the whole mechanism.
Since these stub objects are in case of Hibernate created dynamically at runtime by using the bytecode libraries \textit{javassist} or \textit{CGLIB}, we talk about dynamic proxies. 


\subsubsection{Ensuring eager loading}

As written earlier, it is desired to apply lazy loading of object collections for as many cases as possible. If it is necessary, switching to the eager loading strategy can be done using the following ways:

\begin{itemize}
	\item{change of Hibernate mapping configuration}
	- This modification involves setting the \texttt{lazy=''false''} attribute for collections to be eagerly loaded. 
	It means that all affected 1:N relationships are always loaded together with the parent object. 
	This approach is not very flexible, because no proxies are created and the lazy loading behavior cannot be therefore configured at runtime.

	\item {custom HQL queries or Hibernate criteria that force eager fetching}
	- If HQL queries are used, one can specify eager fetching using the \texttt{fetch} keyword. 
	In case of using the Criteria Query API, the \texttt{setFetchMode()} method with its fetch mode attribute set to \texttt{FetchMode.EAGER} does the job.
	This alternative is more flexible than the previous one since lazy object initialization, which is set by default, is overridden by the eager fetch mode. 
	The created proxy objects call the associated real objects to fetch necessary data. 
	The drawback of this solution is the necessity to create custom HQL queries or Hibernate criteria for each entity that will cause collections to be lazy-loaded. As the created queries can differ a lot, it is very difficult to apply the solution in a generic way.
	

	\item {usage of the \texttt{Hibernate.initialize()} method} 
		- This method is used to initialize lazy-loaded collections. Its parameter takes an object that is to be fetched to the parent object.
		The method also ensures that all already initialized objects will be omitted.
		The power of the \texttt{Hibernate.initialize()} method lies in its universality. When used together with Java Reflection, a universal solution enforcing eager loading of collections for any POJO object can be achieved. 
		It can be used even after the session is closed.

\end{itemize}

%U lazy loadingu jde o usporu nepotrebnych dotazu, tim padem
%mensi rezii a tim urychleni behu aplikace. Lazy loading je proto
%uprednostnovan a pro kolekce, tedy vazby 1:N, je v Hibernate
%nastaven jako default. Pokud vsak potrebujeme pristoupit
%k objektum v kolekci lazily loaded objektu, dostaneme LazyInitializationException,
%protoze objekty nebyly do kolekce nacteny.
%
%Naproti tomu eager loading pri prvnim pristupu k objektu vytvori
%instance pro vsechny objekty, na ktere ma ziskany objekt vazbu. Tento
%approach je pametove narocnejsi, zpomaluje
%beh aplikace a mel by se pouzit jen v pripadech, kdy jsme
%si jisti, ze budeme potrebovat informace o associated objects.
%has negative side-effects on performance.
%
%V aplikaci portalu je z techto rozumnych reasons snaha pouzit lazy
%loading, kde to jen jde. V common situations neni eager loading
%zapotrebi, a proto... 
%
%Pro indexaci dat je zapotrebi fetch data z databaze vcetne
%vsech objektu, ktere k nim belong, aby se do indexu zachytily
%informace i z techto podrazenych objektu.

Based on the aforementioned possibilities 


\section{Indexing algorithm}



\section{Solr configuration}

\subsection{Handling synonyms}


\section{Periodic indexing}

There are two ways to add an article to the EEG/ERP Portal group: either indirectly by filling in the form on the EEG/ERP Portal website or directly from LinkedIn.

In the first case, articles can be indexed immediately after publishing them because their times of publishing are known due to the interaction with EEG/ERP Portal. 
The last published article in the LinkedIn group can be fetched by the means of LinkedIn REST API and then modified by the indexer so that information about the article can be added to the index.

The latter case is a bit more complicated as there is no interaction with the EEG/ERP Portal. 
So in order to index all LinkedIn articles, the article data must be retrieved first.
It can be done by using LinkedIn REST API calls to receive their object representation.
Since there is no available information of when articles are uploaded to LinkedIn, the REST calls must be done periodically.
This way periodic indexing of all articles published on LinkedIn can be achieved.
Although the obvious disadvantage of periodic indexing is the existence of a delay between publishing times of some articles and their indexing times (which is equal to the indexing period in the worst case), this method assures that all articles get indexed in the end.

\section{Scheduling in Spring}

The Spring Framework has a native support of task scheduling and asynchronous
calls. Since its version 3.0, methods can be scheduled and also run
asynchronously by using annotations, namely the @Scheduled and @Async
annotations. The first mentioned annotation, when added to a method,
makes the method schedulable by Spring. Usage of this annotation is
restricted to the void methods with no parameters. The @Scheduled
annotation has to contain a piece of metadata to tell Spring how to
plan the method scheduling. Currently there are three available attributes
for the @Scheduled annotation, from which the most flexible option
is specifying a cron expression to trigger a task as shown on the
following lines:

\begin{lstlisting}[language=Java]
@Scheduled(cron=* 0 22 * * SAT-SUN)
public void indexAll()
\end{lstlisting}


This way, the method indexAll() will be scheduled to run every at
10 pm only on Saturday and Sunday. Cron syntax allows a user to create
more sophisticated scheduling scenarios, but discussing the syntax
is out of scope of this work. Since Spring is using Quartz as a scheduler
under the covers, an interested reader can find all necessary information
about the syntax in the Quartz documentation \cite{QuartzDoc}.


The @Async annotation is used to mark the methods to be invoked asynchronously.
It is very easy to use for methods having void return values:

\begin{lstlisting}[language=Java]
@Async
public void indexLinkedIn()
\end{lstlisting}


In order to enable annotation-based scheduling it is necessary to
add a new element in the application context file as well as the task
namespace to which the element belongs.

\begin{lstlisting}
<xmlns:task="... http://www.springframework.org/schema/task" 
xsi:schemaLocation="... http://www.springframework.org/schema/task/spring-task.xsd">
...
<task:annotation-driven executor="indexingExecutor" scheduler="indexingScheduler"/>
\end{lstlisting}


The annotation-driven element requires executer and scheduler attributes
to be set to handle tasks represented by methods marked by @Async
and @Scheduled annotations, respectively.

\begin{lstlisting}
<task:executor id="indexingExecutor" pool-size="5"/> 
<task:scheduler id="indexingScheduler" pool-size="1"/>
\end{lstlisting}




,,Notice that an executor reference is provided for handling those
tasks that correspond to methods with the @Async annotation, and the
scheduler reference is provided for managing those methods annotated
with @Scheduled.``


\section{User Interface}

\subsection{Search Form}

\subsubsection{Autocomplete}

\subsection{Search Results}

\input{chapters/testing}

\chapter{Conclusion}

% *************** Bibliography ***************
\bibliographystyle{ieeetr}
\bibliography{bibliography}

%\printbibliography
%\begin{thebibliography}{}
%\end{thebibliography}

\end{document}
